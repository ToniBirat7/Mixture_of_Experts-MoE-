## **Resources**

**Articles**

[DataCamp](https://www.datacamp.com/blog/mixture-of-experts-moe)

[Wikipedia: Mixture of Experts](https://en.wikipedia.org/wiki/Mixture_of_experts)

[IBM: Mixture of Experts](https://www.ibm.com/think/topics/mixture-of-experts)

[Medium: Understanding Mixture of Experts](https://medium.com/@mne/explaining-the-mixture-of-experts-moe-architecture-in-simple-terms-85de9d19ea73)

[Hugging Face Blog: Mixture of Experts](https://huggingface.co/blog/moe)

**Youtube Videos**

[Standford: Language Modeling From Scratch](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)

[Vizuara](https://www.youtube.com/watch?v=v7U21meXd6Y)

[Standford:Lecture](https://www.youtube.com/watch?v=LPv1KfUXLCo)

[Playlist](https://www.youtube.com/playlist?list=PLkyja1QUaq_CQ8z97kPvoDbipnIKpBrgM)